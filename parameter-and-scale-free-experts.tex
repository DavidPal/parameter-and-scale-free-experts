\documentclass{article}

\usepackage{fullpage,amssymb,amsmath,amsthm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\KL}[2]{D\left(#1 \middle\| #2 \right)}
\DeclareMathOperator{\Regret}{Regret}

\begin{document}

\title{Parameter- and Scale-Free Algorithms for Learning with Expert Advice}
\author{Francesco Orabona \and D\'avid P\'al}
\maketitle

\section{Introduction}

Learning with Expert Advice (LEA) is an online problem where in each round an
algorithm chooses a point $x_t$ in the probability simplex $\Delta_N = \{x \in
\R^N ~:~ x \ge 0, \norm{x}_1 = 1 \}$ and as a response it receives a loss
vector $\ell_t \in \R^N$ and suffers a loss $\langle \ell_t, x_t \rangle$.
Algorithm's goal is to be competitive with a strategy that in each round
chooses the same point $u \in \Delta_N$.
The difference of their losses, after $T$ rounds, is called
\emph{regret with respect to u}. Formally,
$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, x_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle \; .
$$

We wish to construct an algorithm that receives as an input \emph{prior
distribution} $\pi \in \Delta_N$ and an positive integer $N$ and satisfies
\begin{equation}
\label{equation:parameter-scale-free-bound}
\forall T \ge 0 \quad \forall u \in \Delta_N \qquad \Regret_T(u) \le \sqrt{\KL{u}{\pi} \sum_{t=1}^T \norm{\ell_t}_\infty^2} \; .
\end{equation}
In particular, the algorithm does \emph{not} receive neither $u$, $T$,
$\sum_{t=1}^T \norm{\ell_t}_\infty^2$ nor $\max_{1 \le t \le T}
\norm{\ell_t}_\infty$, nor learning rate nor step size.
Thus the algorithm must be \emph{parameter-free} and \emph{scale-free}.

The bound \eqref{equation:parameter-scale-free-bound} is in contrast with existing
algorithms for LEA. Existing algorithms either assume that $\norm{\ell_t}_\infty \le 1$
for all $t \ge 0$ or the bound on their regret is looser, e.g., $O\left( \sqrt{\ln N \sqrt{\sum_{t=1}^T \norm{\ell_t}_\infty^2}} \right)$.

\section{Parameter- and Scale-Free Algorithms}

\end{document}
